{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import psycopg2\n",
        "import sqlalchemy as sa\n",
        "from sqlalchemy.engine import url as sa_url\n",
        "from sqlalchemy import create engine\n",
        "import pymysql\n",
        "\n",
        "def create_gp_engine():\n",
        "    username = \"user\"\n",
        "    password = \"pwd\"\n",
        "    drivername='postgresql+psycopg2'\n",
        "    host = \"10.10.10.10\"\n",
        "    port = 6433\n",
        "    database = 'gpafiniti'\n",
        "\n",
        "    url = sa_url.URL(drivername=drivername, host=host, port=port, database=database,\n",
        "                    usernamesusername, password=password)\n",
        "\n",
        "    return sa.create_engine(url)\n"
      ],
      "metadata": {
        "id": "hNOQJJZtedDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai={}\n",
        "ai['user'] ='user'\n",
        "ai['password'] = 'pwd'\n",
        "ai['name'] = 'AI'\n",
        "ai['host'] = '10.10.10.10'\n",
        "ai['port'] = 3367\n",
        "\n",
        "prod={}\n",
        "prod['user'] ='user'\n",
        "Prod['password'] = 'pwd'\n",
        "prod['name'] = 'PROD'\n",
        "prod['host'] = '19.19.19.19'\n",
        "prod['port'] = 3307"
      ],
      "metadata": {
        "id": "Xds0-g0RedDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create engine arch_server(database) :\n",
        "\n",
        "    conn = psycopg2.connect(\n",
        "        host = \"10.10.10.10\",\n",
        "        database = database,\n",
        "        user = \"user\",\n",
        "        password = \"pwd\",\n",
        "        port = 6433)\n",
        "\n",
        "    return conn\n",
        "\n",
        "def run_query(engine,query):\n",
        "\n",
        "    df = pd.read_sql(query,engine)\n",
        "    #df. columns = df.column.map(str. Lower)\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "Yfvsj4a_edDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql_engine_arch = create_engine_arch_server(database = \"gpafiniti\")"
      ],
      "metadata": {
        "id": "LyF6gRryedDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"dim tariff_sgmnt_code, a.dim mno type,\n",
        "\n",
        " \n",
        "\n",
        "lim tariff_id::varchar, a.dim tariff_type id\n",
        "\n",
        "   \"\"\"\n",
        "data = run_query(sql_engine_arch, query)"
      ],
      "metadata": {
        "id": "LSymAg4OedDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = \"\"\"select sub.feata,\n",
        "    case when sub.cnt < 100 then 0\n",
        "    when pct between 0 and 0.1 then 1\n",
        "    when pct between 0.1 and 0.2 then 2\n",
        "    when pct between 0.2 and 0.3 then 3\n",
        "    when pct between 0.3 and 0.4 then 4\n",
        "    when pct between 0.4 and 0.5 then 5\n",
        "    when pct between 0.5 and 0.75 then 6\n",
        "    when pet between 0.75 and 1 then 7\n",
        "    else null end as sgmnt from (\n",
        "select feata , round(avg(is_sale),2) pct,count(*) cnt from afint.sme svn\n",
        "where call_date >= '2022-06-15' and call_date < '2022-69-30' and kampanya tipi in (‘campa', ‘campb’)\n",
        "and agent_id is not null and agent_id <> ‘’ and agent id <>‘ ' and isrelevant = 1\n",
        "group by 1 order by 2 desc) sub;\n",
        "\"\"\"\n",
        "\n",
        "sgmt_data=run_query(sql_engine_arch, query2)\n",
        "print(sgmt_data.head())\n"
      ],
      "metadata": {
        "id": "nxSZa57DedDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.merge(data,sgmt_data,left_on = 'feata', right_on = 'featb', how = 'left')\n",
        "data['sgmnt'] = data['sgmnt'].astype('object')\n",
        "data.drop(columns = ['featb'], inplace = True)\n",
        "print (data.info())\n",
        "data['featc'] = data['featc'].astype('object')\n",
        "num_colz = list(data._get_numeric_data().columns)\n",
        "num_colz = list(set(num_colz)-set(['featc']))\n",
        "print(num_colz)\n",
        "exc_cols = ['feata']\n",
        "cat_colz = list(set(data.columns)-set(num_colz)-set(exc_cols))\n",
        "\n",
        "##### Important because above set and List operations changed the order of columns. Now we are ordering them according to column ordering\n",
        "num_cols = [x for x in data.columns if x in num_colz]\n",
        "cat_cols = [x for x in data.columns if x in cat_colz]"
      ],
      "metadata": {
        "id": "XlnB6ScDedDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [x for x in cat_cols if x not in ['sgmnt','other']]:\n",
        "    #print(data[i].value_counts())\n",
        "    print(list(data[i][data.groupby(i)[i].transform('size')<10]))\n",
        "    data[i] = data[i].replace(list(data[i][data.groupby(i)[i].transform('size')<10]),'smalls')\n",
        "    print(data[i].value_counts())"
      ],
      "metadata": {
        "id": "EnOS8fBKedDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "drop_cols = ['featd', 'feate']\n",
        "for i in drop_cols:\n",
        "    data[i+' flg'] = np.where(data[i] == 999999999, 1,0)\n",
        "    print (data[i+' flg'])"
      ],
      "metadata": {
        "id": "VXVO7x0OedDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(columns = drop_cols, inplace=True)"
      ],
      "metadata": {
        "id": "1-YVi6R4edDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['other'] = data['other'].str.lower()\n",
        "data['feataa'] = data['other'].str.contains('xyz')*1\n",
        "print(data['feataa'].value_counts())\n",
        "data['featbb'] = data['other'].str.contains('mnb')*1\n",
        "print (data['featbb'].value_counts())\n",
        "data['sinirsiz'] = data['other'].str.contains('sinirsiz')*1\n",
        "print(data['sinirsiz'].value_counts())\n",
        "data[ 'social'] = data['other'].str.contains('social')*1\n",
        "print (data['social'].value_counts())\n",
        "\n",
        "data['other'] = data['other'].str.replace(' mb', 'mb')\n",
        "print (data['other'].value_counts())\n",
        "data['split_other'] = data['other'].str.split(\" \")\n",
        "print(data['split_other'])"
      ],
      "metadata": {
        "id": "YSBSr2iAedDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check creatin a column from a column with None values, a very basic operation Like summing with a fixed value\n",
        "#print (sum(pd.isnull (data[' split other’ ])))\n",
        "#data['split_other'].replace('None*,np.nan, inplace = True)\n",
        "#print(data[‘split_other' }][pd.isnull(data[‘split_other']) == True])\n",
        "#data[' split_other'].replace(‘None*,np.nan, inplace = True)\n",
        "#print(sum(pd. isnull(data[' split _other'])))\n",
        "#print(data['split_other'].value_counts())\n",
        "#data[' split other test'] = List(filter(lambda k : 'mb' in k, data['split_other'][pd.isnull(data['split_other'])==False]))\n",
        "#data['split_other_test’] = [k for k in (data['split_other' ][pd.isnull(data[‘split_other'])==False]) if 'mb' in k]\n",
        "\n",
        "df_list = []\n",
        "for i in range(len(data)):\n",
        "    #print(i)\n",
        "    #print (data. loc[i, 'split_other'])\n",
        "    if data.loc[i, 'split_other'] is None:\n",
        "        df_list.append([0])\n",
        "    else:\n",
        "        app_list = []\n",
        "        app_list = [k for k in data['split_other'].iloc[i] if 'mb' in k]\n",
        "        app_list_new = [sub.replace(',','') for sub in app_list]\n",
        "        app_list_new = [sub.replace('mb','') for sub in app_list_new]\n",
        "        app_list_new = [float(i) for i in app_list_new]\n",
        "        df_list.append(app_list_new)\n",
        "    #print(df_List)\n",
        "data['mb other'] = df_list\n",
        "print(data['mb_other'].tail(10))"
      ],
      "metadata": {
        "id": "1aK8c3qEedDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['ab_other_sum'] = [sum(a) for a in data['ab_other']]\n",
        "#data['ab_other_sum'] = data['ab_other'].apply(sum)\n",
        "print(data['ab_other_sum'].tail())\n"
      ],
      "metadata": {
        "id": "tu7QM-jeedDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(columns = ['ab_other', 'split_other', 'other'] ,axis = 1, inplace = True)\n",
        "for col in ['feate', 'featf']:\n",
        "    data[col] = data[col].astype('object')"
      ],
      "metadata": {
        "id": "GEeckHxFedDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cat_cols)\n",
        "cat_cols.extend(['feat_e', 'feat_f', 'social'])\n",
        "cat_cols.remove('other')\n",
        "print(data.info())\n",
        "print(cat_cols)\n",
        "\n",
        "num_cols.extend(['ab_other_sum'])\n",
        "num_cols = [x for x in num_cols if x not in ['featg', 'feath']]\n"
      ],
      "metadata": {
        "id": "g0hI2W10edDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[cat_cols] = data[cat_cols].fillna(data[cat_cols].mode().iloc[0])\n",
        "data[num_cols] = data[num_cols].fillna(data{num_cols].median())\n"
      ],
      "metadata": {
        "id": "ncNyjQWVedDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['feata', 'featb','social']:\n",
        "    data[col] = data[col].astype('object')\n",
        "print (data.info())"
      ],
      "metadata": {
        "id": "mv5h7r24edDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting a NA threshold and dropping vars according to that. Also fix, empty and duplicate columns are deleted.\n",
        "fix_cols = []\n",
        "for col in data.columns:\n",
        "    if data[col].nunique() == 1:\n",
        "        fix_cols.append(col)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "emp_cols = []\n",
        "for col in data.columns:\n",
        "    if data[col].count() > 0:\n",
        "        pass\n",
        "    else:\n",
        "        emp_cols.append(col)\n",
        "\n",
        "print(len(fix_cols))\n",
        "print(len(emp_cols))\n",
        "\n",
        "def getDuplicateColumns(df):\n",
        "    duplicateColumnNames = set()\n",
        "    duplicateColumDict = dict()\n",
        "    for x in range(df.shape[1]):\n",
        "        col = df.iloc[:,x]\n",
        "        for y in range(x+l, df.shape[1]):\n",
        "            otherCol = df.iloc[:,y]\n",
        "            if col.equals(otherCol) :\n",
        "                duplicateColumnNames.add(df.columns.values[y])\n",
        "                duplicateColumnDict[df.columns[x]] = df.columns[y]\n",
        "\n",
        "    return list(duplicateColumnNames), duplicateColumnDict\n",
        "\n",
        "dupColNames, dupColDict = getDuplicateColumns(data)\n",
        "\n",
        "for key, value in dupColDict.items():\n",
        "    print(\"key = {}, value = {}\".format(key, value))\n",
        "dropcols = list()\n",
        "\n",
        "dropcols.extend([x for x in fix_cols if x not in dropcols])\n",
        "dropcols.extend([x for x in emp_cols if x not in dropcols])\n",
        "dropcols.extend([x for x in dupColNames if x not in dropcols])\n",
        "print(dropcols)\n",
        "\n",
        "data.drop(columns = dropcols, axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "wcXw27xLedDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (cat_cols)\n",
        "print (num_cols)\n",
        "from statistics import mean\n",
        "\n",
        "def get_redundant_pairs(df):\n",
        "    pairs_to_drop = set()\n",
        "    cols = df.columns\n",
        "    for i in range(0,df.shape[1]):\n",
        "        for j in range(0,i+1):\n",
        "            pairs_to_drop.add((cols[i], cols[j]))\n",
        "    return pairs_to_drop\n",
        "\n",
        "def get_triangle(df):\n",
        "    au_corr = df.corr().abs().unstack()\n",
        "    labels_to_drop = get_redundant_pairs(df)\n",
        "    au_corr = au_corr.drop(labels=labels_to_drop)\n",
        "    #print(au_corr)\n",
        "    au_corr = au_corr.sort_values(ascending=False)\n",
        "    return au_corr\n",
        "\n",
        "left_cols = num_cols.copy()\n",
        "corr_cols = []\n",
        "elim rows = []\n",
        "mean dict = {}\n",
        "row = 0\n",
        "while get_triangle(data[left_cols])[0:1].values > 0.8:\n",
        "\n",
        "    #print(Len(left_cols))\n",
        "    sub_df = get_triangle(data[left_cols])\n",
        "    filter_df_1 = [t for t in sub df.index if (t[0] == sub df.index[0][0]) or (t[1] == sub_df.index[0][0])]\n",
        "    filter_df_2 = [t for t in sub df.index if (t[0] == sub df.index[0][1]) or (t[1] == sub_df.index[0][1])]\n",
        "    #print(np.nanmean(sub_df[filter_df 1].values))\n",
        "    if np.nanmean(sub_df[filter_df_1].values) > np.nanmean(sub_df[filter_df_2].values):\n",
        "        corr_cols.append(sub_df[0:1].index[0][0])\n",
        "        left_cols.remove(sub_df[0:1].index[0][0])\n",
        "    else:\n",
        "        corr_cols.append(sub_df[0:1].index[0][1])\n",
        "        left_cols.remove(sub_df[0:1].index[0][1])\n",
        "    elim_rows.append(sub_df[0:1])\n",
        "    mean_dict[row]=[(sub_df[0:1].index[0][0], np.nanmean(sub_df[filter_df_1].values)), (sub_df[0:1].index[0][1], np.nanmean(sub_df[filter_df_2].values))]\n",
        "    row = row + 1\n",
        "    #print(len(left_cots))\n",
        "\n",
        "print(corr_cols)\n",
        "print(elim_rows)\n",
        "print(mean_dict)\n",
        "\n",
        "num_cols = [x for x in num cols if x not in corr_cols]\n",
        "#print(sub_df[sub_df.apply(lambda x: ‘curr_revenue_amt_pre_cr' in x)])"
      ],
      "metadata": {
        "id": "oGht2bDGedDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols.remove('mno_type')\n",
        "column_headers = [cat_cols[x].split()[-1] for x in range(len(cat_cols))]\n"
      ],
      "metadata": {
        "id": "eOvMIu2pedDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_copy = data.copy()\n",
        "\n",
        "def gnumeric_func(data, columns):\n",
        "    data[columns] = data[columns].applyl(lambda x: pd.factorize(x)[0])\n",
        "# return data\n",
        "gnumeric_func(data_copy, column_headers)\n",
        "print(data.info())"
      ],
      "metadata": {
        "id": "I538eRQtedDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "from scipy.stats import chi2\n",
        "import pandas as pd\n",
        "final_df= pd.DataFrame(columns=('column1', 'column2','cramers_V'))\n",
        "def cramers_v(x,y):\n",
        "    confusion_matrix = pd.crosstab(data_copy[x],data_copy[y])\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 =chi2/n\n",
        "    r,k = confusion_matrix.shape\n",
        "    phi2corr = max(0,phi2-((k-1)*(r-1))/(n-1))\n",
        "    rcorr = r-((r-1)**2)/(n-1)\n",
        "    kcorr = k-((k-1)**2)/(n-1)\n",
        "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n",
        "i=0\n",
        "for coll in column_headers:\n",
        "    if coll!='on off':\n",
        "        for col2 in column_headers:\n",
        "            if (col2!='on off' and coll!=col2):\n",
        "                x = cramers_v(coll,col2)\n",
        "                final_df.loc[i]=[col1,col2,x]\n",
        "                i=i+1"
      ],
      "metadata": {
        "id": "3CuvW_8-edDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "%matplotlib inline\n",
        "\n",
        "# plt.imshow(final_df, cmap=\"hot', interpolation='nearest')\n",
        "# plt.show()\n",
        "\n",
        "# final_df.plot(x='cl')\n",
        "\n",
        "temp_df= final_df.pivot(index='column1', columns='column2', values='cramers_V')\n",
        "pyplot.figure(figsize=(20,20))\n",
        "hm=sns.heatmap(temp_df, vmin=0, vmax=1, linewidths=0.1)\n",
        "\n",
        "hm.figure.savefig(\"output.png\")"
      ],
      "metadata": {
        "id": "9Ti_itPMedDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.sort_values(by=['cramers_V','columnl'], ascending=False).iloc[0:5]"
      ],
      "metadata": {
        "id": "JkQ8URQmedDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep = data.copy()\n",
        "print (data[num_cols].head())\n",
        "print (data[cat_cols].head())\n",
        "print (data_prep.info())"
      ],
      "metadata": {
        "id": "NPIvpaPpedDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_cols)\n",
        "print(cat_cols)\n",
        "#print(data_prep[cat_cols].info())\n",
        "data prep['featc'].replace(0, 'none', inplace=True)\n",
        "data_prep.drop(columns = ['featd'], inplace= True)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "numeric transformer = Pipeline(steps=[('scl', MinMaxScaler(feature_range = (0,1)))])\n",
        "categoric transformer = Pipeline(steps=[('ohe', OneHotEncoder(sparse = True, handle_unknown = 'ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers = [('cat', categoric transformer, cat_cols), ('num', numeric_transformer, num_cols)]\n",
        "                                                ,remainder = 'passthrough')\n",
        "\n",
        "preprocessor_scl = ColumnTransformer(transformers = [('cat', 'passthrough', cat_cols), ('num', numeric transformer, num cols)]\n",
        "                                                ,remainder = ‘passthrough')\n",
        "\n",
        "data_prep_scl = preprocessor_scl.fit_transform(data_prep)\n",
        "\n",
        "data_p = preprocessor. fit_transform(data_prep)\n",
        "#data_p = all_pipe.named steps[‘prep'].fit_transform(data_prep)\n",
        "\n",
        "print(preprocessor.transformers_[0][1].named_steps['ohe'].get feature names(cat_cols))"
      ],
      "metadata": {
        "id": "Xd7fGfUtedDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (cat_cols)\n",
        "print(data_prep[cat_cols].head())\n",
        "print (data prep.info())"
      ],
      "metadata": {
        "id": "8cBGWBPYedD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols_ohe = list(preprocessor.transformers_[0][1].named_steps['ohe'].get_feature_names(cat_cols))\n",
        "columns_prep = cat_cols_ohe + num_cols\n",
        "print(columns_prep)\n",
        "data_p_df = pd.DataFrame(data_p, columns = columns_prep)\n",
        "print(data_p_df.info())\n",
        "\n",
        "‘##### It doesn’t matter if we only applied scaling with transformer, it changes column orders, gets cat first then num as scaled\n",
        "columns_prep_scl = cat_cols + num cols\n",
        "data_prep_scl_df = pd.DataFrame(data_prep_scl, columns = columns_prep_scl)\n",
        "\n",
        "for i in num_cols:\n",
        "    data_prep_scl_df[i] = data_prep_scl_df[i].astype('float')\n",
        "\n",
        "print(data_prep_scl_df.info())"
      ],
      "metadata": {
        "id": "xNTS0RVledD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## TECHNIQUES LIKE PCA FAMD NEED SCALING!!! 11111\n",
        "from sklearn.decomposition import PCA\n",
        "from prince import FAMD\n",
        "\n",
        "### It runs with mixed vars, one hot encoded data is not mixed\n",
        "#famd = FAMD(n_iter = 160, copy = True, check input = True, engine = ‘auto’, random state = 42)\n",
        "#famd.fit(data_p_df)\n",
        "\n",
        "famd = FAMD(n_iter = 500, copy = True, check_input = True, engine = 'auto', random state = 42)\n",
        "fand_p_df = famd.fit_transform(data_prep_scl_df)\n"
      ],
      "metadata": {
        "id": "k1yBmwKWedD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### referred to ‘bpostance/clustering mixed data’ for use of silhouette scores. He uses it wit df itself, and predicted labels not a distance matrix\n",
        "##### I made some research and It seems valid to me (if you want to use distance matrix you can select metric as precomputed. Euclidean etc means how to calculate)\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import cm\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "results = dict()\n",
        "k_cand = [2,3,4,5,6,7,8,9, 10,11, 12, 13,14, 15, 16]\n",
        "\n",
        "for k in k cand:\n",
        "    kmeans = KMeans(n_clusters = k, random state = 0, init = 'k-means++').fit(famd_p_df)\n",
        "    score0 = kmeans.inertia_\n",
        "    scorel = silhouette_score(famd_p_df, kmeans.labels_ , metric = 'euclidean')\n",
        "    score2 = silhouette_score(famd_p_df, kmeans.labels_ , metric = 'correlation')\n",
        "    results[k] = {'k':kmeans, 's0':score0, 's1':scorel, 's2':score2}\n",
        "\n",
        "fix,axs = plt.subplots(1,2,sharex=True, figsize=(10,3))\n",
        "axs[0].plot([i for i in results.keys()], [i['s0'] for i in results.values()], 'o-', label = 'Inertia')\n",
        "axs[1].plot([i for i in results.keys()], [i['sl'] for i in results.values()], 'o-', label = 'Euclidean')\n",
        "axs[1].plot([i for i in results.keys()], [i['s2'] for i in results.values()], 'o-', label = 'Correlation')\n",
        "\n",
        "for ax in axs:\n",
        "    ax.set_xticks(k_cand)\n",
        "    ax.set_xlabel('K')\n",
        "    ax.legend()"
      ],
      "metadata": {
        "id": "5ymXOtt1edD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### We don't prefer pca with one hot encoded data but for comparison with famd i added it(next 2-3 cells)\n",
        "\n",
        "pca = PCA()\n",
        "pca. fit(data_p_df)\n",
        "pca.explained_variance_ratio_\n",
        "\n",
        "plt.figure(figsize = (10,8))\n",
        "plt.plot(range(1,59), pca.explained_variance_ratio_.cumsum(), marker = 'o', Linestyle = '--')\n",
        "plt.title('Explained Variance by components')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')"
      ],
      "metadata": {
        "id": "9m-rPVuVedD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 15)\n",
        "pca_p_df = pca.fit_transform(data_p_df)\n",
        "print (pca_p_df)"
      ],
      "metadata": {
        "id": "hXLiv79vedD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = dict()\n",
        "k_cand = [2,3,4,5,6,7,8,9, 10,11, 12, 13,14, 15, 16]\n",
        "\n",
        "for k in k_cand:\n",
        "    kmeans = KMeans(n_clusters = k, random_state = 0, init = 'k-means++').fit(pca_p_df)\n",
        "    score0 = kmeans.inertia_\n",
        "    scorel = silhouette_score(pca_p_df, kmeans.labels_, metric = 'euclidean')\n",
        "    score2 = silhouette_score(pca_p_df, kmeans.labels_, metric = 'correlation')\n",
        "    results[k] = {'k':kmeans, 's0':score0, 's1':scorel, 's2':score2}\n",
        "\n",
        "fix,axs = plt.subplots(1,2,sharex=True, figsize=(10,3))\n",
        "\n",
        "axs[0].plot([i for i in results.keys()], [i['s0'] for i in results.values()], 'o-', label = 'Inertia')\n",
        "axs[1].plot([i for i in results.keys()], [i['sl'] for i in results.values()], 'o-', label = 'Euclidean')\n",
        "axs[1].plot([i for i in results.keys()], [i['s2'] for i in results.values()], 'o-', label = 'Correlation')\n",
        "\n",
        "for ax in axs:\n",
        "    ax.set_xticks(k_cand)\n",
        "    ax.set_xlabel('K')\n",
        "    ax.legend()"
      ],
      "metadata": {
        "id": "fM5jEK9HedD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for using one hot data in kmeans\n",
        "results = dict()\n",
        "k_cand = [2,3,4,5,6,7,8,9, 10,11, 12, 13,14,15, 16]\n",
        "\n",
        "for k in k cand:\n",
        "    kmeans = KMeans(n_clusters = k, random_state = 0).fit(data_p_df)\n",
        "    #kmeans = KMeans(n_clusters = k, random state = 10, init = 'k-means++').fit(data pdf)\n",
        "    score0 = kmeans.inertia_\n",
        "    scorel = silhouette_score(data_p_df, kmeans.labels_, metric = 'euclidean')\n",
        "    score2 = silhouette_score(data_p_df, kmeans.labels_, metric = 'correlation')\n",
        "    results[k] = {'k':kmeans, 's0':score0, 'sl':scorel, 's2':score2}\n",
        "\n",
        "fix,axs = plt.subplots(1,2,sharex=True, figsize=(10,3))\n",
        "axs[0].plot([i for i in results.keys()], [i['s0'] for i in results.values()], 'o-', label = 'Inertia')\n",
        "axs[1].plot([i for i in results.keys()], [i['sl'] for i in results.values()], 'o-', label = 'Euclidean')\n",
        "axs[1].plot([i for i in results.keys()], [i['s2'] for i in results.values()], 'o-', label = 'Correlation')\n",
        "\n",
        "for ax in axs:\n",
        "    ax.set_xticks(k_cand)\n",
        "    ax.set_xlabel('K')\n",
        "    ax.legend()"
      ],
      "metadata": {
        "id": "7laLh1VuedD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### among trials above, most suitable for this problem is famd approach with 5-6 clusters\n",
        "k_cand_v2 = [3,5,6,7,9]\n",
        "fig, axs = plt.subplots(len(k_cand_v2),2,figsize=(12,12))\n",
        "\n",
        "for e,k in enumerate (k_cand_v2):\n",
        "    kmeans = KMeans(n_clusters=k, random_state = 0).fit(famd_p_df)\n",
        "\n",
        "    cdict = {i:cm.Set1(i) for i in np.unique(kmeans.labels_)}\n",
        "    silhouette_vals = silhouette_samples(famd_p_df,kmeans.labels_)\n",
        "    y_lower = 0\n",
        "    y_upper = 0\n",
        "\n",
        "    for i, cluster in enumerate(np.unique(kmeans.labels_)):\n",
        "        cluster_silhouette_vals = silhouette_vals[kmeans.labels_ == cluster]\n",
        "        cluster_silhouette vals.sort()\n",
        "        y_upper += len(cluster_silhouette_vals)\n",
        "        axs[e,0].barh(range(y_lower,y_upper), cluster_silhouette_vals, height=1, color=cdict[cluster])\n",
        "        axs[e,0].text(-0.03,(y_lower+y_upper)/2,str(i))\n",
        "\n",
        "        y_lower += len(cluster_silhouette_vals)\n",
        "        avg_score = np.mean(silhouette_vals)\n",
        "        axs[e,0].axvline(avg score, linestyle = '--', color = 'black')\n",
        "\n",
        "        axs[e,0].set_yticks([])\n",
        "        axs[e,0].set_xlim([-0.1,1])\n",
        "        axs[e,0].set_xlabel('Silhouette coefficient values')\n",
        "        axs[e,0].set_ylabel('Cluster labels')\n",
        "        axs[e,0].set_title('Silhouette plot for the various clusters')\n",
        "\n",
        "    #pca for visualization\n",
        "    pca_df = PCA(n_components = 2).fit_transform(famd_p_df)\n",
        "    pca_df = pd.DataFrame(pca_df)\n",
        "    pca_df['k'] = kmeans.labels_\n",
        "\n",
        "    for cluster in np.unique(kmeans.labels_):\n",
        "        axs[e,1].scatter(x=pca_df.where(pca_df['k']==cluster)[0],\n",
        "                        y = pca_df.where(pca_df['k']==cluster)[1],\n",
        "                        color = cdict[cluster],\n",
        "                        label=cluster)\n",
        "\n",
        "    axs[e,1].scatter(x=kmeans.cluster_centers_[:,0],\n",
        "                    y=kmeans.cluster_centers_[:,1],\n",
        "                    marker = 'x', color='black', s=180)\n",
        "\n",
        "    axs[e,1].legend(bbox_to_anchor=(1,1))\n",
        "    axs[e,1].set_title(f\"KMeans\\n$k$ = {k}\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "\"\"\" This part is for visualizing 2 feature clusters\n",
        "    results = data_p_df.copy()\n",
        "    results['k'] = kmeans.labels_\n",
        "\n",
        "    for cluster in np.unique(kmeans.labels_):\n",
        "        axs[e,1].scatter(x=pca_df.where(pca_df['k']==cluster)[0],\n",
        "                        y=pca_df.where(pca_df['k']==cluster)[1],\n",
        "                        color = cdict[cluster],\n",
        "                        label=cluster)\n",
        "        axs[e,1].scatter(x=kmeans.cluster_centers_[:,0],\n",
        "                        y=kmeans.cluster_centers_[:,1],\n",
        "                        marker = 'x', color='black', s=180)\n",
        "\n",
        "        axs[e,1].legend(bbox_to_anchor=(1,1))\n",
        "        axs[e,1].set_title(f\"KMeans\\n$k$ = {k}\")\n",
        "        plt.tight_layout()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_fZfbm2MedD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=10, random state = 0).fit(data_p_df)\n",
        "data['clust_kmeans_wo_famd'] = kmeans.labels_\n",
        "\n",
        "kmeans2 = KMeans(n_clusters=7, random state = 0).fit(famd_p_df)\n",
        "data['clust_kmeans_w_famd'] = kmeans2.labels_\n",
        "print (data.head())"
      ],
      "metadata": {
        "id": "gbXp9mzXedD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kprototypes is using mixed data. And silhouette scores are using distance matrix between features. So it cant be computed with mixezd data.\n",
        "# You can use a distance matrix calculation for mixed data to enable silhouette scores again\n",
        "from kmodes.kprototypes import KPrototypes\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "results = dict()\n",
        "k_cand = [2,3,4,5,6,7,8,9,10,11,12,13,14,15, 16,17, 18,19]\n",
        "lbl = LabelEncoder()\n",
        "data_prep_scl_lbl = data_prep_scl_df[cat_cols].apply(lbl.fit_transform)\n",
        "data_prep_scl_lbl = pd.concat([data_prep_scl_lbl, data_prep_scl_df[num_cols]], axis = 1)\n",
        "print(data_prep_scl_lbl.head())\n",
        "\n",
        "data_prep_array = data_prep_scl_lbl.values\n",
        "print(data_prep_array)\n",
        "cat_idx = data_prep_scl_lbl.columns.get_indexer(cat_cols).tolist()\n",
        "print (cat_idx)\n",
        "\n",
        "for k in k_cand:\n",
        "    kproto = KPrototypes(n_clusters = k, random_state = 0, max iter = 300, init = 'Huang', n jobs = 12)\n",
        "    kproto.fit_predict(data_prep_array, categorical = cat_idx)\n",
        "\n",
        "    score0 = kproto.cost_\n",
        "    results[k] = {'k':kproto, 's0':score0}\n",
        "\n",
        "fig,axs = plt.subplots(1,1,sharex-False, figsize=(10,3))\n",
        "axs.plot([i for i in results.keys()], [i['s0'] for i in results.values()], 'o-', label = 'Inertia')\n",
        "axs.set_xticks(k_cand)\n",
        "axs.set_xlabel('K')\n",
        "axs.legend()"
      ],
      "metadata": {
        "id": "zUzxWPgsedD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kproto5 = KPrototypes(n_clusters = 6, random_state = 0, max_iter = 300, init = 'Huang', n_jobs = 12)\n",
        "kproto5.fit_predict(data_prep_array, categorical = cat_idx)\n",
        "data['clust_kprototype'] = kproto5.labels_\n",
        "print (data.head())\n"
      ],
      "metadata": {
        "id": "tcQNSvIJedD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "spect = SpectralClustering(n_clusters = 5, random state = 0, n_jobs = 12).fit(famd_p_df)\n",
        "data['clust_spectral_5'] = spect.labels_\n",
        "spect2 = SpectralClustering(n_clusters = 10, random_state = 0, n_jobs = 12).fit(famd_p_df)\n",
        "data['clust_spectral_10'] = spect2.labels_"
      ],
      "metadata": {
        "id": "1BQpQFt7edD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as sch\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from scipy.cluster import hierarchy\n",
        "\n",
        "plt.figure(figsize = (17,10)\n",
        "plt.title('Dendogram')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel ('Euclidean Distances')\n",
        "dendrogram = sch.dendrogram(sch.linkage(famd_p_df, method = 'ward'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "se4Baou4edD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc = AgglomerativeClustering(n_clusters = 7, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(famd_p_df)\n",
        "data['clust_hierarchical'] = y_hc\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "OQzKHtmqedD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hdbscan import HDBSCAN\n",
        "\n",
        "hdb = HDBSCAN(min_cluster_size = 25, min samples = 5).fit(famd_p_df)\n",
        "data['cluster_hdbscan'] = hdb.labels_\n",
        "print(data['cluster_hdbscan'].value counts())\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "Ebs0BRK2edD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "push_data = data[['dim_tariff_id','clust_kmeans_wo_famd','clust_kmeans_w_famd', 'clust_kprototype', 'clust_spectral_5',\n",
        "                'clust_spectral_10', 'clust_hierarchical','cluster_hdbscan']]\n",
        "\n",
        "print(push_data.head())\n",
        "sql_engine_arch = create_engine_arch_server(database = \"gpafiniti\")\n",
        "con = create_gp_engine()\n",
        "push_data.to sql(if_exists = 'replace', name= 'tariff_clustering', con = con, schema= 'afiniti_aida', index = False)"
      ],
      "metadata": {
        "id": "4gDcXyIoedD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn=\"mysql+pymysql://%s:%s@%s:%s/%s\" %(ai['user'],ai['password'],ai['host'],ai['port'], 'smexplorerdata')\n",
        "engine=create_engine(conn)\n",
        "push_data.to_sql(if_exists = 'replace', name= 'tariff_clustering', con = engine, schema= 'smexplorerdata', index = False)\n"
      ],
      "metadata": {
        "id": "1koTclEsedD2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}